{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "0it [00:00, ?it/s]\n",
      "TqdmDeprecationWarning: Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm(...))`.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics import log_loss\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from simhash import Simhash\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from matplotlib import pyplot\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm_pandas(tqdm())\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_types = [\"what\", \"how\", \"why\", \"is\", \"which\", \"can\", \"i\", \"who\", \"do\", \"where\", \"if\", \"does\", \"are\", \"when\", \"should\", \"will\", \"did\", \"has\", \"would\", \"have\", \"was\", \"could\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def submit(p_test):\n",
    "\n",
    "    sub = pd.DataFrame()\n",
    "\n",
    "    sub['test_id'] = df_test['test_id']\n",
    "    sub['is_duplicate'] = p_test\n",
    "\n",
    "    sub.to_csv('simple_xgb.csv', index=False)   \n",
    "\n",
    "def get_inverse_freq(inverse_freq, count, min_count=2):\n",
    "\n",
    "    if count < min_count:   \n",
    "        return 0\n",
    "    else:\n",
    "        return inverse_freq\n",
    "\n",
    "def get_tf(text):\n",
    "\n",
    "    tf = {}\n",
    "\n",
    "    for word in text:\n",
    "        tf[word] = text.count(word)/len(text)\n",
    "\n",
    "    return tf\n",
    "\n",
    "def tuple_similarity(q1_words, q2_words):\n",
    "\n",
    "    if len(q1_words) == 0 or len(q2_words) == 0:\n",
    "        return 0\n",
    "\n",
    "    common_words = len(set(q1_words).intersection(set(q2_words)))\n",
    "    all_words = len(set(q1_words).union(set(q2_words)))\n",
    "\n",
    "    return common_words/all_words\n",
    "\n",
    "def get_ne_score(row):\n",
    "\n",
    "    q1_words = str(row.question1).lower().split()\n",
    "    q2_words = str(row.question2).lower().split()\n",
    "\n",
    "    # all_words_score = np.sum([weights.get(w, 0) for w in q1_words]) + np.sum([weights.get(w, 0) for w in q2_words])\n",
    "\n",
    "    q1 = nlp(unicode(str(row[\"question1\"]), \"utf-8\"))\n",
    "    q2 = nlp(unicode(str(row[\"question2\"]), \"utf-8\"))\n",
    "\n",
    "    q1_ne = q1.ents\n",
    "    q2_ne = q2.ents\n",
    "\n",
    "    q1_ne = set([str(i) for i in q1_ne])\n",
    "    q2_ne = set([str(i) for i in q2_ne])\n",
    "\n",
    "    if len(q1_ne) == 0:\n",
    "        q1_ne_ratio = 0\n",
    "    else:\n",
    "        q1_ne_ratio = len(q1_ne)/len(q1_words)\n",
    "\n",
    "    if len(q2_ne) == 0:\n",
    "        q2_ne_ratio = 0\n",
    "    else:\n",
    "        q2_ne_ratio = len(q2_ne)/len(q2_words)\n",
    "\n",
    "    common_ne = list(q1_ne.intersection(q2_ne))\n",
    "    # common_ne_weights = np.sum([weights.get(w, 0) for w in common_ne])\n",
    "\n",
    "    if len(q1_ne) + len(q2_ne) == 0:\n",
    "        common_ne_score = 0\n",
    "    else:\n",
    "       common_ne_score = len(common_ne)/(len(q1_words) + len(q2_words) - len(common_ne))\n",
    "\n",
    "    return pd.Series({\n",
    "        \"q1_ne_ratio\": q1_ne_ratio,\n",
    "        \"q2_ne_ratio\": q2_ne_ratio,\n",
    "        \"ne_diff\": abs(q1_ne_ratio - q2_ne_ratio),\n",
    "        \"ne_score\": common_ne_score\n",
    "    })\n",
    "\n",
    "\n",
    "def basic_nlp(row):\n",
    "\n",
    "    # q1_tf = get_tf(q1_words)\n",
    "    # q2_tf = get_tf(q2_words)\n",
    "    \n",
    "    q1_words = str(row.question1).lower().split()\n",
    "    q2_words = str(row.question2).lower().split()\n",
    "\n",
    "    #modify this!\n",
    "    if len(q1_words) == 0 or len(q2_words) == 0:\n",
    "        return 0\n",
    "\n",
    "    common_words = list(set(q1_words).intersection(q2_words))\n",
    "    \n",
    "    common_words_score = np.sum([weights.get(w, 0) for w in common_words])\n",
    "    all_words_score = np.sum([weights.get(w, 0) for w in q1_words]) + np.sum([weights.get(w, 0) for w in q2_words]) - common_words_score\n",
    "\n",
    "    hamming_score = sum(1 for i in zip(q1_words, q2_words) if i[0]==i[1])/max(len(q1_words), len(q2_words))\n",
    "\n",
    "    jacard_score =  len(common_words)/(len(q1_words) + len(q2_words) - len(common_words))  \n",
    "    cosine_score = len(common_words)/(pow(len(q1_words),0.5)*pow(len(q2_words),0.5))\n",
    "\n",
    "    bigrams_q1 = set(ngrams(q1_words, 2))\n",
    "    bigrams_q2 = set(ngrams(q2_words, 2))\n",
    "    common_bigrams = len(bigrams_q1.intersection(bigrams_q2))\n",
    "    if common_bigrams == 0:\n",
    "        bigram_score = 0\n",
    "    else:\n",
    "        bigram_score = common_bigrams/(len(bigrams_q1.union(bigrams_q2)))    \n",
    "\n",
    "    trigrams_q1 = set(ngrams(q1_words, 3))\n",
    "    trigrams_q2 = set(ngrams(q2_words, 3))\n",
    "    common_trigrams = len(trigrams_q1.intersection(trigrams_q2))\n",
    "    if common_trigrams == 0:\n",
    "        trigram_score = 0\n",
    "    else:\n",
    "        trigram_score = common_trigrams/(len(trigrams_q1.union(trigrams_q2)))    \n",
    "\n",
    "    # sequence1 = get_word_bigrams(q1_words)\n",
    "    # sequence2 = get_word_bigrams(q2_words)\n",
    "\n",
    "    # try:\n",
    "    #     simhash_diff = Simhash(sequence1).distance(Simhash(sequence2))/64\n",
    "    # except:\n",
    "    #     simhash_diff = 0.5\n",
    "\n",
    "    q1 = nlp(unicode(str(row[\"question1\"]), \"utf-8\"))\n",
    "    q2 = nlp(unicode(str(row[\"question2\"]), \"utf-8\"))\n",
    "\n",
    "    q1_ne = q1.ents\n",
    "    q2_ne = q2.ents\n",
    "\n",
    "    q1_ne = set([str(i) for i in q1_ne])\n",
    "    q2_ne = set([str(i) for i in q2_ne])\n",
    "\n",
    "    if len(q1_ne) == 0:\n",
    "        q1_ne_ratio = 0\n",
    "    else:\n",
    "        q1_ne_ratio = len(q1_ne)/len(q1_words)\n",
    "\n",
    "    if len(q2_ne) == 0:\n",
    "        q2_ne_ratio = 0\n",
    "    else:\n",
    "        q2_ne_ratio = len(q2_ne)/len(q2_words)\n",
    "\n",
    "    common_ne = list(q1_ne.intersection(q2_ne))\n",
    "    # common_ne_weights = np.sum([weights.get(w, 0) for w in common_ne])\n",
    "\n",
    "    if len(q1_ne) + len(q2_ne) == 0:\n",
    "        common_ne_score = 0\n",
    "    else:\n",
    "       common_ne_score = len(common_ne)/(len(q1_words) + len(q2_words) - len(common_ne))\n",
    "\n",
    "    pos_hash = {}\n",
    "    common_pos = []\n",
    "\n",
    "    for word in q1:\n",
    "        if word.tag_ not in pos_hash:\n",
    "            pos_hash.update({word.tag_ : [word.text]})\n",
    "        else:\n",
    "            pos_hash[word.tag_].append(word.text)\n",
    "\n",
    "    for word in q2:\n",
    "        if word.tag_ not in pos_hash:\n",
    "            continue\n",
    "        if word.text in pos_hash[word.tag_]:\n",
    "            common_pos.append(word.text)\n",
    "\n",
    "    common_pos_score = np.sum([weights.get(w, 0) for w in common_pos])\n",
    "    all_pos_score = np.sum([weights.get(w, 0) for w in q1_words]) + np.sum([weights.get(w, 0) for w in q2_words]) - common_pos_score\n",
    "\n",
    "    q1_pronouns_count = 0\n",
    "    q2_pronouns_count = 0\n",
    "\n",
    "    for word in q1:\n",
    "        if str(word.tag_) == \"PRP\":\n",
    "            q1_pronouns_count += 1\n",
    "\n",
    "    for word in q2:\n",
    "        if str(word.tag_) == \"PRP\":\n",
    "            q2_pronouns_count += 1\n",
    "\n",
    "    pronouns_diff = abs(q1_pronouns_count - q2_pronouns_count)\n",
    "\n",
    "    q1_nc = q1.noun_chunks\n",
    "    q2_nc = q2.noun_chunks\n",
    "\n",
    "    q1_nc = set([str(i) for i in q1_nc])\n",
    "    q2_nc = set([str(i) for i in q2_nc])\n",
    "\n",
    "    common_nc = len(q1_nc.intersection(q2_nc))\n",
    "\n",
    "    if len(q1_nc) + len(q2_nc) == 0:\n",
    "        common_nc_score = 0\n",
    "    else:\n",
    "       common_nc_score = common_nc/(len(q1_nc) + len(q2_nc) - common_nc)\n",
    "\n",
    "    fw_q1 = q1_words[0]\n",
    "    fw_q2 = q2_words[0]\n",
    "\n",
    "    if fw_q1 == fw_q2 and fw_q1 in question_types:\n",
    "        question_type_same = 1\n",
    "    else:\n",
    "        question_type_same = 0\n",
    "\n",
    "    try:\n",
    "        q1_quotes = len(re.findall(r'\\\"(.+?)\\\"', row[\"question1\"]))\n",
    "    except:\n",
    "        q1_quotes = 0\n",
    "\n",
    "    try:\n",
    "        q2_quotes = len(re.findall(r'\\\"(.+?)\\\"', row[\"question2\"]))\n",
    "    except:\n",
    "        q2_quotes = 0\n",
    "\n",
    "    # if len(q1_ne) == 0:\n",
    "    #     q1_ne_hash_freq = 1\n",
    "    # else:\n",
    "    #     hash_key1 = hash(\"-\".join(set([str(i).lower() for i in q1_ne])))\n",
    "\n",
    "    #     if hash_key1 not in hash_table_ne:\n",
    "    #         q1_ne_hash_freq = 1\n",
    "    #     else:\n",
    "    #         q1_ne_hash_freq = hash_table_ne[hash_key1]\n",
    "\n",
    "    # if len(q2_ne) == 0:\n",
    "    #     q2_ne_hash_freq = 1\n",
    "    # else:\n",
    "    #     hash_key2 = hash(\"-\".join(set([str(i).lower() for i in q2_ne])))\n",
    "\n",
    "    #     if hash_key2 not in hash_table_ne:\n",
    "    #         q2_ne_hash_freq = 1\n",
    "    #     else:\n",
    "    #         q2_ne_hash_freq = hash_table_ne[hash_key2]\n",
    "\n",
    "    try:\n",
    "        q1_sents = len(nltk.tokenize.sent_tokenize(row.question1))\n",
    "    except:\n",
    "        q1_sents = 1\n",
    "    try:\n",
    "        q2_sents = len(nltk.tokenize.sent_tokenize(row.question2))\n",
    "    except:\n",
    "        q2_sents = 1\n",
    "\n",
    "    q1_exclaim = sum([1 for i in str(row.question1) if i == \"!\"])\n",
    "    q2_exclaim = sum([1 for i in str(row.question2) if i == \"!\"])\n",
    "\n",
    "    q1_question = sum([1 for i in str(row.question1) if i == \"?\"])\n",
    "    q2_question = sum([1 for i in str(row.question2) if i == \"?\"])\n",
    "\n",
    "    hash_key1 = hash(str(row[\"question1\"]).lower())\n",
    "    if hash_key1 in hash_table:\n",
    "        q1_hash_freq = hash_table[hash_key1]\n",
    "    else:\n",
    "        q1_hash_freq = 1\n",
    "\n",
    "    hash_key2 = hash(str(row[\"question2\"]).lower())\n",
    "    if hash_key2 in hash_table:\n",
    "        q2_hash_freq = hash_table[hash_key2]\n",
    "    else:\n",
    "        q2_hash_freq = 1\n",
    "\n",
    "    # if hash_key1 in pos_hash_table:\n",
    "    #     q1_dup_ratio = pos_hash_table[hash_key1]/q1_hash_freq\n",
    "    # else:\n",
    "\n",
    "    #     q1_dup_ratio = 0\n",
    "\n",
    "    # if hash_key2 in pos_hash_table:\n",
    "    #     q2_dup_ratio = pos_hash_table[hash_key2]/q2_hash_freq\n",
    "    # else:\n",
    "    #     q2_dup_ratio = 0\n",
    "\n",
    "    spacy_sim = q1.similarity(q2)\n",
    "\n",
    "    return pd.Series({\n",
    "\n",
    "        \"weighted_word_match_ratio\" : common_words_score/all_words_score,\n",
    "        \"weighted_word_match_diff\": all_words_score - common_words_score, \n",
    "        \"weighted_word_match_sum\": common_words_score,\n",
    "        \"jacard_score\": jacard_score,\n",
    "        \"hamming_score\": hamming_score,\n",
    "        \"cosine_score\": cosine_score,\n",
    "        \"bigram_score\": bigram_score,\n",
    "        \"trigram_score\": trigram_score,\n",
    "        \"pos_score\": common_pos_score/all_pos_score,\n",
    "        # \"simhash_diff\": simhash_diff,\n",
    "        \"question_type_same\": question_type_same,\n",
    "        \"q1_stops\": len(set(q1_words).intersection(stops))/len(q1_words),\n",
    "        \"q2_stops\": len(set(q2_words).intersection(stops))/len(q2_words),\n",
    "        \"q1_len\": len(str(row.question1)),\n",
    "        \"q2_len\": len(str(row.question2)),\n",
    "        \"len_diff\": abs(len(str(row.question1)) - len(str(row.question2))),\n",
    "        \"len_avg\": (len(str(row.question1)) + len(str(row.question2)))/2,\n",
    "        \"q1_sents\": q1_sents,\n",
    "        \"q2_sents\": q2_sents,\n",
    "        \"sents_diff\": abs(q1_sents - q2_sents),\n",
    "        \"q1_words\": len(q1_words),\n",
    "        \"q2_words\": len(q2_words),\n",
    "        \"words_diff\": abs(len(q1_words) - len(q2_words)),\n",
    "        \"words_avg\": (len(q1_words) + len(q2_words))/2,\n",
    "        \"q1_caps_count\": sum([1 for i in str(row.question1) if i.isupper()]),\n",
    "        \"q2_caps_count\": sum([1 for i in str(row.question2) if i.isupper()]),\n",
    "        \"q1_exclaim\": q1_exclaim,\n",
    "        \"q2_exclaim\": q2_exclaim,\n",
    "        \"exclaim_diff\": abs(q1_exclaim - q2_exclaim),\n",
    "        \"q1_question\": q1_question,\n",
    "        \"q2_question\": q2_question,\n",
    "        \"question_diff\": abs(q1_question - q2_question),\n",
    "        \"ne_score\": common_ne_score,\n",
    "        \"nc_score\": common_nc_score,\n",
    "        \"q1_ne_ratio\": q1_ne_ratio,\n",
    "        \"q2_ne_ratio\": q2_ne_ratio,\n",
    "        \"ne_diff\": abs(q1_ne_ratio - q2_ne_ratio),\n",
    "        \"q1_quotes\": q1_quotes,\n",
    "        \"q2_quotes\": q2_quotes,\n",
    "        \"quotes_diff\": abs(q1_quotes - q2_quotes),\n",
    "        # \"q1_ne_hash_freq\": q1_ne_hash_freq,\n",
    "        # \"q2_ne_hash_freq\": q2_ne_hash_freq,\n",
    "        # \"chunk_hash_diff\": abs(q1_ne_hash_freq - q2_ne_hash_freq),\n",
    "        \"q1_hash_freq\": q1_hash_freq,\n",
    "        \"q2_hash_freq\": q2_hash_freq,\n",
    "        \"q_freq_avg\": (q1_hash_freq + q2_hash_freq)/2,\n",
    "        \"freq_diff\": abs(q1_hash_freq - q2_hash_freq),\n",
    "        \"spacy_sim\": spacy_sim,\n",
    "        \"q1_pronouns_count\": q1_pronouns_count,\n",
    "        \"q2_pronouns_count\": q2_pronouns_count,\n",
    "        \"pronouns_diff\": pronouns_diff\n",
    "        # \"q1_dup_ratio\": q1_dup_ratio,\n",
    "        # \"q2_dup_ratio\": q2_dup_ratio,\n",
    "        # \"q1_q2_dup_ratio_avg\": (q1_dup_ratio + q2_dup_ratio)/2\n",
    "    })\n",
    "\n",
    "def neighbor_intersection(row):\n",
    "\n",
    "    q1_neighbors = graph[row[\"question1\"]]\n",
    "    q2_neighbors = graph[row[\"question2\"]]\n",
    "\n",
    "    common_neighbors = set(q1_neighbors).intersection(q2_neighbors)\n",
    "\n",
    "    return len(common_neighbors)\n",
    "\n",
    "def get_q1_second_degree_freq(row):\n",
    "\n",
    "    q1_neighbors = graph[row[\"question1\"]]\n",
    "\n",
    "    q1_second_degree_neighbors = []\n",
    "    for i in q1_neighbors:\n",
    "        q1_second_degree_neighbors += graph[i]\n",
    "\n",
    "    return len(set(q1_second_degree_neighbors))\n",
    "\n",
    "def get_q2_second_degree_freq(row):\n",
    "\n",
    "    q2_neighbors = graph[row[\"question2\"]]\n",
    "\n",
    "    q2_second_degree_neighbors = []\n",
    "    for i in q2_neighbors:\n",
    "        q2_second_degree_neighbors += graph[i]\n",
    "\n",
    "    return len(set(q2_second_degree_neighbors))\n",
    "\n",
    "\n",
    "def second_degree_intersection(row):\n",
    "\n",
    "    q1_neighbors = graph[row[\"question1\"]]\n",
    "    q2_neighbors = graph[row[\"question2\"]]\n",
    "\n",
    "    q1_second_degree_neighbors = []\n",
    "    for i in q1_neighbors:\n",
    "        q1_second_degree_neighbors += graph[i]\n",
    "\n",
    "    q2_second_degree_neighbors = []\n",
    "    for i in q2_neighbors:\n",
    "        q2_second_degree_neighbors += graph[i]\n",
    "    \n",
    "    common_second_degree_neighbors = set(q1_second_degree_neighbors).intersection(set(q2_second_degree_neighbors))\n",
    "\n",
    "    return len(common_second_degree_neighbors)\n",
    "\n",
    "# def pos_neighbor_intersection(row):\n",
    "\n",
    "#     if row[\"question1\"] in pos_graph and row[\"question2\"] in pos_graph:\n",
    "#         q1_neighbors = pos_graph[row[\"question1\"]]\n",
    "#         q2_neighbors = pos_graph[row[\"question2\"]]\n",
    "\n",
    "#         common_neighbors = set(q1_neighbors).intersection(q2_neighbors)\n",
    "\n",
    "#         return len(common_neighbors)/(len(q1_neighbors) + len(q2_neighbors) - len(common_neighbors))\n",
    "\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "def get_word_bigrams(words):\n",
    "\n",
    "    ngrams = []\n",
    "\n",
    "    for i in range(0, len(words)):\n",
    "        if i > 0:\n",
    "            ngrams.append(\"%s %s\"%(words[i-1], words[i]))\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "def generate_hash_freq(row):\n",
    "\n",
    "    hash_key1 = hash(row[\"question1\"].lower())\n",
    "    hash_key2 = hash(row[\"question2\"].lower())\n",
    "\n",
    "    if hash_key1 not in hash_table:\n",
    "        hash_table[hash_key1] = 1\n",
    "    else:\n",
    "        hash_table[hash_key1] += 1\n",
    "\n",
    "    if hash_key2 not in hash_table:\n",
    "        hash_table[hash_key2] = 1\n",
    "    else:\n",
    "        hash_table[hash_key2] += 1\n",
    "\n",
    "def generate_duplicate_freq(row):\n",
    "\n",
    "    hash_key1 = hash(row[\"question1\"].lower())\n",
    "    hash_key2 = hash(row[\"question2\"].lower())\n",
    "\n",
    "    if hash_key1 not in pos_hash_table and row[\"is_duplicate\"] == 1:\n",
    "        pos_hash_table[hash_key1] = 1\n",
    "    elif hash_key1 not in pos_hash_table and row[\"is_duplicate\"] == 0:\n",
    "        pos_hash_table[hash_key1] = 0\n",
    "    elif hash_key1 in pos_hash_table and row[\"is_duplicate\"] == 1:\n",
    "        pos_hash_table[hash_key1] += 1\n",
    "    # elif hash_key1 in pos_hash_table and row[\"is_duplicate\"] == 0:\n",
    "    #     pass\n",
    "\n",
    "    if hash_key2 not in pos_hash_table and row[\"is_duplicate\"] == 1:\n",
    "        pos_hash_table[hash_key2] = 1\n",
    "    elif hash_key2 not in pos_hash_table and row[\"is_duplicate\"] == 0:\n",
    "        pos_hash_table[hash_key2] = 0\n",
    "    elif hash_key2 in pos_hash_table and row[\"is_duplicate\"] == 1:\n",
    "        pos_hash_table[hash_key2] += 1\n",
    "    # elif hash_key1 in pos_hash_table and row[\"is_duplicate\"] == 0:\n",
    "    #     pass\n",
    "\n",
    "def generate_positive_graph(row):\n",
    "\n",
    "    hash_key1 = row[\"question1\"]\n",
    "    hash_key2 = row[\"question2\"]\n",
    "        \n",
    "    if row[\"is_duplicate\"] == 1:\n",
    "        if hash_key1 not in pos_graph:\n",
    "            pos_graph[hash_key1] = [hash_key2]\n",
    "        elif hash_key1 in pos_graph:\n",
    "            pos_graph[hash_key1].append(hash_key2)\n",
    "\n",
    "        if hash_key2 not in pos_graph:\n",
    "            pos_graph[hash_key2] = [hash_key1]\n",
    "        elif hash_key2 in pos_graph:\n",
    "            pos_graph[hash_key2].append(hash_key1)\n",
    "\n",
    "def generate_graph_table(row):\n",
    "\n",
    "    hash_key1 = row[\"question1\"]\n",
    "    hash_key2 = row[\"question2\"]\n",
    "        \n",
    "    if hash_key1 not in graph:\n",
    "        graph[hash_key1] = [hash_key2]\n",
    "    elif hash_key1 in graph:\n",
    "        graph[hash_key1].append(hash_key2)\n",
    "\n",
    "    if hash_key2 not in graph:\n",
    "        graph[hash_key2] = [hash_key1]\n",
    "    elif hash_key2 in graph:\n",
    "        graph[hash_key2].append(hash_key1)\n",
    "\n",
    "def augment_rows():\n",
    "\n",
    "    new_graph = graph\n",
    "\n",
    "    for q1 in graph:\n",
    "\n",
    "        q2_list = graph[q1]\n",
    "        \n",
    "        for i in q2_list:\n",
    "            for j in q2_list:\n",
    "                if i != j:                    \n",
    "                    if j not in graph[i]:\n",
    "                        new_graph[i].append(j)\n",
    "\n",
    "    # new_df_train = df_train[[\"question1\", \"question2\", \"is_duplicate\"]]\n",
    "\n",
    "    # for i in new_graph:\n",
    "\n",
    "def generate_ne_freq(row):\n",
    "\n",
    "    q1 = nlp(unicode(str(row[\"question1\"]), \"utf-8\"))\n",
    "    q2 = nlp(unicode(str(row[\"question2\"]), \"utf-8\"))\n",
    "\n",
    "    q1_ne = q1.ents\n",
    "    q2_ne = q2.ents\n",
    "\n",
    "    q1_ne = \"-\".join(set([str(i).lower() for i in q1_ne]))\n",
    "    q2_ne = \"-\".join(set([str(i).lower() for i in q2_ne]))\n",
    "    \n",
    "    hash_key1 = hash(q1_ne)\n",
    "    hash_key2 = hash(q2_ne)\n",
    "\n",
    "    if hash_key1 not in hash_table_ne:\n",
    "        hash_table_ne[hash_key1] = 1\n",
    "    else:\n",
    "        hash_table_ne[hash_key1] += 1\n",
    "\n",
    "    if hash_key2 not in hash_table_ne:\n",
    "        hash_table_ne[hash_key2] = 1\n",
    "    else:\n",
    "        hash_table_ne[hash_key2] += 1\n",
    "\n",
    "def oversample(x_train):\n",
    "\n",
    "    neg_train = x_train[x_train.is_duplicate == 0]\n",
    "    pos_train = x_train[x_train.is_duplicate == 1]\n",
    "\n",
    "    #Oversampling negative class\n",
    "    p = 0.165\n",
    "    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1 #How much times greater is the train ratio than actual\n",
    "\n",
    "    while scale > 1:\n",
    "        neg_train = pd.concat([neg_train, neg_train])\n",
    "        scale -=1\n",
    "    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "\n",
    "    return pd.concat([pos_train, neg_train])\n",
    "\n",
    "#When plotted a histogram of degrees, only -1,1 and 2 are observed. Which means either you're max 2 degree separated or you're separate(with 5 as a cutoff). \n",
    "#Add (number of second degree connections) and its intersection as a feature\n",
    "def bfs(q_node, q_search, separation):\n",
    "\n",
    "    if separation > 5:\n",
    "        return -1\n",
    "\n",
    "    if len(graph[q_node]) > 0:\n",
    "        \n",
    "        shortest_res = 32768\n",
    "\n",
    "        if q_search in graph[q_node]:\n",
    "            return separation\n",
    "        else:\n",
    "\n",
    "            for i,j in enumerate(graph[q_node]):\n",
    "\n",
    "                if i > 5:\n",
    "                    return shortest_res\n",
    "\n",
    "                bfs_res = bfs(j, q_search, separation + 1)\n",
    "\n",
    "                if bfs_res != -1 and bfs_res < shortest_res:\n",
    "                    shortest_res = bfs_res\n",
    "\n",
    "            return shortest_res\n",
    "\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def initialize_bfs(row):\n",
    "\n",
    "    q1 = row[\"question1\"]\n",
    "    q2 = row[\"question2\"]\n",
    "\n",
    "    shortest_res = 32768\n",
    "\n",
    "    for i in graph[q1]:\n",
    "        if i != q2:\n",
    "            res = bfs(i, q2, 1)\n",
    "\n",
    "            if res != -1 and res < shortest_res:\n",
    "                shortest_res = res\n",
    "\n",
    "    if shortest_res == 32768:\n",
    "        return -1\n",
    "    else:\n",
    "        return shortest_res\n",
    "\n",
    "def augment_test(row):\n",
    "    global new_df_test\n",
    "\n",
    "    #map q1 with dups of q2\n",
    "    if row[\"question2\"] in pos_graph:\n",
    "        new_rows = pd.DataFrame()\n",
    "        q2_dups = pos_graph[row[\"question2\"]]\n",
    "        new_rows[\"question2\"] = [i for i in q2_dups]\n",
    "        new_rows[\"question1\"] = row[\"question1\"]    \n",
    "        new_rows[\"test_id\"] = row[\"test_id\"]\n",
    "        new_df_test = pd.concat([new_df_test, new_rows])\n",
    "\n",
    "    #map q2 with dups of q1\n",
    "    if row[\"question1\"] in pos_graph:\n",
    "        new_rows = pd.DataFrame()\n",
    "        q1_dups = pos_graph[row[\"question1\"]]\n",
    "        new_rows[\"question1\"] = [i for i in q1_dups]\n",
    "        new_rows[\"question2\"] = row[\"question2\"]    \n",
    "        new_rows[\"test_id\"] = row[\"test_id\"]\n",
    "        new_df_test = pd.concat([new_df_test, new_rows])\n",
    "\n",
    "# def run_xgb(x_train, x_valid, y_train, y_valid):\n",
    "def run_xgb(x_train, x_test, x_label):\n",
    "\n",
    "    # x_train = pd.concat([pos_train, neg_train]) #Concat positive and negative\n",
    "    # y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist() #Putting in 1 and 0\n",
    "\n",
    "    # x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n",
    "\n",
    "    # Set our parameters for xgboost\n",
    "    params = {}\n",
    "    params['objective'] = 'binary:logistic'\n",
    "    params['eval_metric'] = 'logloss'\n",
    "    params['eta'] = 0.05\n",
    "    params['max_depth'] = 6\n",
    "    params['silent'] = 1\n",
    "\n",
    "    d_train = xgb.DMatrix(x_train, label=x_label)\n",
    "    d_test = xgb.DMatrix(x_test)\n",
    "\n",
    "    watchlist = [(d_train, 'train')]\n",
    "\n",
    "    bst = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "\n",
    "    p_test = bst.predict(d_test)\n",
    "\n",
    "    # xgb.plot_importance(bst)\n",
    "    # pyplot.show()\n",
    "\n",
    "    return p_test\n",
    "\n",
    "def run_tsne(pos_train, neg_train, x_test_feat):\n",
    "\n",
    "    x_train = pd.concat([pos_train, neg_train]) #Concat positive and negative\n",
    "    y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist() #Putting in 1 and 0\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n",
    "\n",
    "    df_subsampled = x_train[0:3000]\n",
    "    X = MinMaxScaler().fit_transform(df_subsampled[['z_len1', 'z_len2', 'z_words1', 'z_words2', 'word_match']])\n",
    "    # y = y_train['is_duplicate'].values\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=3,\n",
    "        init='random', # pca\n",
    "        random_state=101,\n",
    "        method='barnes_hut',\n",
    "        n_iter=200,\n",
    "        verbose=2,\n",
    "        angle=0.5\n",
    "    ).fit_transform(X)\n",
    "\n",
    "    trace1 = go.Scatter3d(\n",
    "        x=tsne[:,0],\n",
    "        y=tsne[:,1],\n",
    "        z=tsne[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            sizemode='diameter',\n",
    "            color = y_train,\n",
    "            colorscale = 'Portland',\n",
    "            colorbar = dict(title = 'duplicate'),\n",
    "            line=dict(color='rgb(255, 255, 255)'),\n",
    "            opacity=0.75\n",
    "        )\n",
    "    )\n",
    "\n",
    "    data=[trace1]\n",
    "    layout=dict(height=800, width=800, title='3d embedding with engineered features')\n",
    "    fig=dict(data=data, layout=layout)\n",
    "    py.plot(data, filename='3d_bubble')\n",
    "\n",
    "def validate(training):\n",
    "\n",
    "    training_res = training.pop(\"is_duplicate\")\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(training, training_res, test_size=0.2, random_state=4242, stratify = training_res)\n",
    "\n",
    "    return(x_train, x_valid, y_train, y_valid)\n",
    "\n",
    "def real_testing(df_with_qs, gen_filename):\n",
    "\n",
    "    # Required for initial setup!\n",
    "    # dataframe_modified = df_with_qs.progress_apply(basic_nlp, axis = 1)\n",
    "    old_filename = './old/' + gen_filename \n",
    "    # dataframe_modified.to_csv(old_filename, index = False)\n",
    "\n",
    "    dataframe_modified = pd.read_csv(old_filename).fillna(\"\")\n",
    "\n",
    "    dataframe_modified[\"neighbor_intersection\"] = df_with_qs.apply(neighbor_intersection, axis = 1)\n",
    "\n",
    "    # q1_second_degree_freq = dataframe.apply(get_q1_second_degree_freq, axis = 1)\n",
    "    # q2_second_degree_freq = dataframe.apply(get_q2_second_degree_freq, axis = 1)\n",
    "    # dataframe_modified[\"second_degree_avg\"] = (q1_second_degree_freq + q2_second_degree_freq)/2\n",
    "    # dataframe_modified[\"second_degree_diff\"] = abs(q1_second_degree_freq - q2_second_degree_freq)\n",
    "    # dataframe_modified[\"second_degree_intersection\"] = dataframe.apply(second_degree_intersection, axis = 1)\n",
    "    # dataframe_modified[\"separation\"] = dataframe.progress_apply(initialize_bfs, axis = 1)\n",
    "\n",
    "    new_filename = \"./new/\" + gen_filename    \n",
    "    dataframe_modified.to_csv(new_filename, index=False)\n",
    "    # %reset_selective dataframe_modified \n",
    "\n",
    "def pred_n_submit(x_train, x_label, test_filename, test_id_df, res_filename):\n",
    "\n",
    "    x_test = pd.read_csv(test_filename).fillna(0)\n",
    "\n",
    "    res_1 = run_xgb(x_train, x_test, x_label)\n",
    "    sub = pd.DataFrame()\n",
    "\n",
    "    sub['test_id'] = test_id_df['test_id']\n",
    "    sub['is_duplicate'] = res_1\n",
    "\n",
    "    sub.to_csv(res_filename, index=False)   \n",
    "    # %reset_selective -f x_test_1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./train.csv').fillna(\"\")\n",
    "df_test = pd.read_csv('./test.csv').fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "test_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n",
    "qs = pd.concat([train_qs, test_qs])\n",
    "\n",
    "# tfidf = TfidfVectorizer(max_features = 256, stop_words='english', ngram_range=(1, 1))\n",
    "# tfidf.fit_transform(train_qs[0:2500])\n",
    "\n",
    "#Load up the Weights Dictionary\n",
    "# words = (\" \".join(qs)).lower().split()\n",
    "# counts = Counter(words)\n",
    "# weights = {word: get_inverse_freq(1/(10000 + int(count)), count) for word, count in counts.items()}\n",
    "# with open('word_weights.pickle', 'wb') as handle:\n",
    "#     pickle.dump(weights, handle)\n",
    "with open('word_weights.pickle', 'rb') as handle:\n",
    "    weights = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load up the hashtable\n",
    "# hash_table = {}\n",
    "# df_train.apply(generate_hash_freq, axis = 1)\n",
    "# df_test.apply(generate_hash_freq, axis = 1)\n",
    "# with open('hash_table.pickle', 'wb') as handle:\n",
    "#     pickle.dump(hash_table, handle)\n",
    "with open('hash_table.pickle', 'rb') as handle:\n",
    "    hash_table = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load up the graph!\n",
    "# graph = {}\n",
    "# df_train.apply(generate_graph_table, axis = 1)\n",
    "# df_test.progress_apply(generate_graph_table, axis = 1)\n",
    "# with open('graph.pickle', 'wb') as handle:\n",
    "#     pickle.dump(graph, handle)\n",
    "with open('graph.pickle', 'rb') as handle:\n",
    "    graph = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_testing(df_train, 'x_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_testing(df_test[0:390000], 'x_test_1.csv')\n",
    "real_testing(df_test[390000:780000], 'x_test_2.csv')\n",
    "real_testing(df_test[780000:1170000], 'x_test_3.csv')\n",
    "real_testing(df_test[1170000:1560000], 'x_test_4.csv')\n",
    "real_testing(df_test[1560000:1950000], 'x_test_5.csv')\n",
    "real_testing(df_test[1950000:], 'x_test_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finally!\n",
    "x_train = pd.read_csv('./new/x_train.csv').fillna(0)\n",
    "x_label = df_train.is_duplicate\n",
    "oversample_label = 0\n",
    "\n",
    "if oversample_label == 1:\n",
    "    x_train[\"is_duplicate\"] = df_train.is_duplicate\n",
    "    x_train = oversample(x_train)\n",
    "\n",
    "    x_label = x_train.pop(\"is_duplicate\")\n",
    "    # res_oversampled = run_xgb(x_train_oversampled, x_test, x_label_oversampled)\n",
    "    # submit(res_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.663208\n",
      "Will train until train-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.297538\n",
      "[99]\ttrain-logloss:0.272238\n",
      "[0]\ttrain-logloss:0.663208\n",
      "Will train until train-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.297538\n",
      "[99]\ttrain-logloss:0.272238\n",
      "[0]\ttrain-logloss:0.663208\n",
      "Will train until train-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.297538\n",
      "[99]\ttrain-logloss:0.272238\n",
      "[0]\ttrain-logloss:0.663208\n",
      "Will train until train-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.297538\n",
      "[99]\ttrain-logloss:0.272238\n",
      "[0]\ttrain-logloss:0.663208\n",
      "Will train until train-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.297538\n",
      "[99]\ttrain-logloss:0.272238\n",
      "[0]\ttrain-logloss:0.663208\n",
      "Will train until train-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.297538\n",
      "[99]\ttrain-logloss:0.272238\n"
     ]
    }
   ],
   "source": [
    "pred_n_submit(x_train, x_label, './new/x_test_1.csv', df_test[0:390000], './res_files/res_1.csv')\n",
    "pred_n_submit(x_train, x_label, './new/x_test_2.csv', df_test[390000:780000], './res_files/res_2.csv')\n",
    "pred_n_submit(x_train, x_label, './new/x_test_3.csv', df_test[780000:1170000], './res_files/res_3.csv')\n",
    "pred_n_submit(x_train, x_label, './new/x_test_4.csv', df_test[1170000:1560000], './res_files/res_4.csv')\n",
    "pred_n_submit(x_train, x_label, './new/x_test_5.csv', df_test[1560000:1950000], './res_files/res_5.csv')\n",
    "pred_n_submit(x_train, x_label, './new/x_test_6.csv', df_test[1950000:], './res_files/res_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = pd.read_csv('./res_files/res_1.csv').fillna(\"\")\n",
    "res_2 = pd.read_csv('./res_files/res_2.csv').fillna(\"\")\n",
    "res_3 = pd.read_csv('./res_files/res_3.csv').fillna(\"\")\n",
    "res_4 = pd.read_csv('./res_files/res_4.csv').fillna(\"\")\n",
    "res_5 = pd.read_csv('./res_files/res_5.csv').fillna(\"\")\n",
    "res_6 = pd.read_csv('./res_files/res_6.csv').fillna(\"\")\n",
    "\n",
    "res = pd.concat([res_1, res_2, res_3, res_4, res_5, res_6])\n",
    "res.to_csv(\"./res_files/res_new_piepline_plus_nbrintersect.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
